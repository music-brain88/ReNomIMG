{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import copy\n",
    "import renom as rm\n",
    "import renom_img\n",
    "from renom_img.api.detection.yolo_v2 import Yolov2, create_anchor\n",
    "from renom_img.api.utility.load import parse_xml_detection, load_img\n",
    "from renom.cuda import set_cuda_active\n",
    "set_cuda_active(True)\n",
    "from renom_img.api.utility.augmentation import Augmentation\n",
    "from renom_img.api.utility.augmentation.process import *\n",
    "from renom_img.api.utility.evaluate import EvaluatorDetection\n",
    "from renom_img.api.utility.distributor.distributor import ImageDistributor\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.mkdir('model_logs')\n",
    "date = str(datetime.date(datetime.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_list(filename):\n",
    "    ann=[]\n",
    "    img_path=[]\n",
    "    with open(filename,'r') as f:\n",
    "        for line in f:\n",
    "            line = line[:-1]\n",
    "            line = \"/mnt/research/dataset/VOCdevkit/\"+line\n",
    "            img_path.append(line)\n",
    "            line = line.replace('JPEGImages','Annotations')\n",
    "            line = line.replace('jpg','xml')\n",
    "            ann.append(line)\n",
    "    return img_path,ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_box(pred,actual):\n",
    "    for i in range(len(pred)):\n",
    "        h = actual[i][0]['size'][1]\n",
    "        w = actual[i][0]['size'][0]\n",
    "        if len(pred[i])>0:\n",
    "            for j in range(len(pred[i])):\n",
    "                xmin = pred[i][j]['box'][0] * w\n",
    "                ymin = pred[i][j]['box'][1] * h\n",
    "                xmax = pred[i][j]['box'][2] * w\n",
    "                ymax = pred[i][j]['box'][3] * h\n",
    "                pred[i][j]['box']=[xmin, ymin, xmax, ymax]\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_function(*args):\n",
    "    if len(args)>0:\n",
    "#         calculating mAP\n",
    "        model = args[1]\n",
    "#         saving model\n",
    "        train_list = args[2]\n",
    "        validation_loss_list = args[3]\n",
    "        epoch = args[0]\n",
    "        if len(validation_loss_list)>1:\n",
    "            tmp = copy.deepcopy(validation_loss_list)\n",
    "            current_loss = tmp[-1]\n",
    "            del(tmp[-1])\n",
    "            tmp.sort()\n",
    "            if(current_loss<tmp[0]):\n",
    "                test_dist = ImageDistributor(valid_image)\n",
    "                results = []\n",
    "                for i, (x_img_list, _) in enumerate(test_dist.batch(1, shuffle=False)):\n",
    "                    img_array = np.vstack([load_img(path, model.imsize)[None]\n",
    "                                           for path in x_img_list])\n",
    "                    img_array = model.preprocess(img_array)\n",
    "                    results.extend(model.get_bbox(model(img_array).as_ndarray(),\n",
    "                                                 score_threshold=0.005, nms_threshold=0.45))\n",
    "\n",
    "                predicted = reconstruct_box(results,valid_annotation)\n",
    "                ev = EvaluatorDetection(predicted,valid_annotation)\n",
    "                fp = open('model_logs/yolov2@'+date+'.txt','a+')\n",
    "                fp.write('Epoch: {:03d} Train Loss: {:3.2f}  Valid Loss: {:3.2f} mAP: {:3.2f} \\n'.format(epoch,float(train_list[-1]),float(validation_loss_list[-1]),float(ev.mAP())))\n",
    "                fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image, train_annot = create_list(\"/mnt/research/dataset/VOCdevkit/voc_train.txt\")\n",
    "valid_image, valid_annot = create_list(\"/mnt/research/dataset/VOCdevkit/2007_test.txt\")\n",
    "\n",
    "train_annotation, cmap = parse_xml_detection(train_annot)\n",
    "valid_annotation, _ = parse_xml_detection(valid_annot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_list = create_anchor(train_annotation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug = Augmentation([Shift(15,15),\n",
    "                    Flip(),\n",
    "                    Rotate(),\n",
    "                    ContrastNorm()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Yolov2(cmap, anchor=anchor_list, imsize=(416, 416), load_pretrained_weight=True, train_whole_network=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "total_epoch = 160\n",
    "batch = 8\n",
    "imsize = model.imsize\n",
    "multiscale = [(32*k,32*k) for k in range(10,16)]\n",
    "optimizer = model._opt.__class__\n",
    "augmentation = [str(name.__class__).split('.')[-1] for name in aug._process_list]\n",
    "evaluation_matrix = \"mAP\"\n",
    "dataset = \"PASCAL_VOC_2012\"\n",
    "standard = 76.8\n",
    "load_pretrained=True\n",
    "train_whole=True\n",
    "renom_v = rm.__version__\n",
    "renom_img_v = renom_img.__version__\n",
    "commit_id = str(subprocess.check_output(['git','rev-parse','HEAD']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write hyperparameters to file\n",
    "fp = open('model_logs/yolov2@'+date+'.txt','a+')\n",
    "fp.write('Commit Hash: '+commit_id[2:-3]+'\\nReNom version: '+renom_v+'\\nReNomIMG version: '+renom_img_v)\n",
    "fp.write('\\nExpected score: {:3.2f}\\n'.format(float(standard)))\n",
    "fp.write('\\n===================================================Hyperparameters=======================================================\\n')\n",
    "fp.write('\\nTotal epoch: {:03d}\\nBatch size: {:03d}\\nImage size: ({:03d},{:03d})'.format(total_epoch,batch,imsize[0],imsize[1]))\n",
    "fp.write('\\nMultiscale: '+str(multiscale)+'\\nOptimizer: '+str(optimizer).split('.')[-1]+'\\nAugmentation: '+str(augmentation))\n",
    "fp.write('\\nEvaluation matrix: '+str(evaluation_matrix)+'\\nDataset: '+str(dataset))\n",
    "fp.write('\\nLoad Pretrained weight: '+str(load_pretrained)+'\\nTrain whole network: '+str(train_whole))\n",
    "fp.write('\\n==========================================================================================================================\\n\\n')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_image,train_annotation,valid_image,valid_annotation, augmentation=aug,\n",
    "                epoch=total_epoch,batch_size=batch, imsize_list=[(32*k,32*k) for k in range(10,16)],callback_end_epoch=end_function)\n",
    "fp = open('model_logs/yolov2@'+date+'.txt','a')\n",
    "fp.write('\\nSuccess')\n",
    "fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
